---
title: "kremlin.ru_ru_2024"
description: "A textual dataset based on the contents published on the Russian-language version of the Kremlin’s website (1999-2023)"
author: Giorgio Comai
date: 2024-02-24
last-modified: 2024-02-24
categories: [dataset, Russian institutions, Russian language]
editor: source
---


```{r setup, echo = FALSE}
source(fs::path("..", "dataset_setup_2024.R"))

cas_set_options(
  base_folder = fs::path(fs::path_home_r(), 
                         "R",
                         "castarter_2024"),
  project = "Russian institutions",
  website = "kremlin.ru_ru" 
)

corpus_original_df <- cas_read_db_contents_data() |> 
  dplyr::collect()

corpus_df <- corpus_original_df

website_name <- cas_get_options()[["website"]]
corpus_name <- stringr::str_c(website_name, "_2024")
```

## Scope of this dataset

This textual dataset is based on [kremlin.ru](http://enkremlin.ru/), i.e. the Russian-language version of the official website of the president of the Russian Federation. It includes only its main sections with news and updated; it does not include other sections of the website such legal documents, the Constitution, etc. 

This dataset includes contents published between 31 December 1999 and 31 December 2023, under two Russian presidents: Vladimir Putin and Dmitri Medvedev. 

## Narrative explanation of how this textual dataset was built

Kremlin.ru publishes all of its news items in one ore more of the following sections:

- [transcripts](http://kremlin.ru/events/president/transcripts)
- [Presidential Executive Office](http://kremlin.ru/events/administration)
- [State Council](http://kremlin.ru/events/state-council)
- [Security Council](http://kremlin.ru/events/security-council)
- [Commissions and Councils](http://kremlin.ru/events/councils)
- [news](http://kremlin.ru/events/president/news)

This dataset has been generated by parsing each of these sections, similarly to what would be accomplished by insistently clicking on the "show more" link at the bottom of the relevant index pages until the oldest post has been reached.

Some items are posted in more than one sections with different urls; they however keep the same internal id: a series of up to 5 digits included at the end of each url. For example, the article "Meeting with permanent members of the Security Council" has been posted on 4 February 2011 at both of the following urls:

- http://kremlin.ru/events/president/news/10235
- http://kremlin.ru/events/security-council/10235

In order to prevent duplication of contents, only one of these articles is preserved in the final dataset. Only one of these urls effectively features in the final dataset; for consistency, only the first match, according to the order in which sections are listed above, is kept, which allows to see easily which posts are defined as "transcripts" and usually prefers more specific sections to the generic "news". This choice should be substantively irrelevant for most use cases, as sections are included in the metadata. 

## Metadata




## License information

At the time contents were retrieved, the footer of kremlin.ru as well as the dedicated [copyright page](http://kremlin.ru/about/copyrights) make clear that:

> "all materials published on this website are available with the following license "[Creative Commons Attribution 4.0 International](http://creativecommons.org/licenses/by/4.0/deed.ru)"

This license gives the right to “copy and redistribute the material in any medium or format”, and to “remix, transform, and build upon the material for any purpose, even commercially”, as long as appropriate credit is given to the source and the license is included.

The contents of this dataset - “kremlin.ru_ru” - are distributed within the remits of this license. To the extent that it is possible, the dataset itself is also distributed by its creator, Giorgio Comai, with the same CC-BY license, as well as under the Open Data Commons Attribution license (ODC-BY).


## Dataset cleaning steps

### Duplicated post issues

The dataset has originally been generated by parsing all index pages for the following categories available on the Kremlin's website:

```{r}

cas_read_db_index() |> 
  dplyr::distinct(index_group) |> 
  dplyr::collect() |> 
  knitr::kable()

```

In total, the corpus thus generated has `r scales::number(nrow(corpus_df |> dplyr::filter(date<=end_date)))` items. It appears, however, that starting with 2008 items that are included in more than one category are published with a separate url. These are items published on the same date, with the same title, and exactly (or almost exactly) the same text, yet would still be included each time they appear as they have a separate url. Here are a few examples:

```{r}
corpus_df |> 
  dplyr::mutate(internal_id = stringr::str_extract(string = url,
                                                   pattern = "[[:digit:]]+$")) |> 
  dplyr::group_by(internal_id) |> 
  dplyr::filter(dplyr::n()>1) |>
  dplyr::ungroup() |> 
  dplyr::arrange(date, time, internal_id) |> 
  dplyr::select(date, url, title, internal_id) |> 
  dplyr::slice_head(n = 6) |> #|> View()
  knitr::kable()
```

Luckily, it is easy to notice that such duplicated articles share the same internal id (the numeric part at the end of the url is the same).

For analytical purposes, it makes sense to drop such duplicated items.

```{r}
corpus_deduplicated_df <- corpus_df |> 
    dplyr::mutate(internal_id = stringr::str_extract(string = url,
                                                   pattern = "[[:digit:]]+$")) |> 
  dplyr::distinct(internal_id, .keep_all = TRUE)

corpus_df <- corpus_deduplicated_df
```

Once these duplicated items are removed, the total number of items is effectively `r scales::number(nrow(corpus_deduplicated_df))`. These are the ones that are included in the published dataset and used in further analyses.

```{r eval = FALSE}
corpus_df |> 
  dplyr::group_by(date, text) |> 
  dplyr::add_count() |> 
  dplyr::ungroup() |> 
  dplyr::filter(n>1) |> 
  dplyr::arrange(date, id) 
```

## Dataset cleaning and reordering

The following steps are conducted on the original dataset before exporting:

- ensure all items have a date
- ensure no post following the cut-off date (`r end_date`) is included
- ensure no posts with duplicated id are included
- in line with the [`tif` standard](https://docs.ropensci.org/tif/), create a `doc_id` column (composed of the website base url, the language of the dataset, and the internal id) and set this as the first column of the dataset


```{r cleaning}
corpus_df <- corpus_original_df

corpus_pre_df <- corpus_df 

## ensure dates always present
corpus_df <- corpus_df |> 
  dplyr::filter(is.na(date)==FALSE) 
  
check <- assertthat::assert_that(nrow(corpus_original_df)==nrow(corpus_df), 
                        msg = "rows dropped due to missing dates")

## close dataset at end date
corpus_df <- corpus_df |> 
  dplyr::filter(date<=end_date) 


## deduplicate consistently by section order

section_order_v <- c("http://kremlin.ru/events/president/transcripts/",
  "http://kremlin.ru/events/administration/",
  "http://kremlin.ru/events/state-council/",
  "http://kremlin.ru/events/security-council/", 
  "http://kremlin.ru/events/councils/",
  "http://kremlin.ru/events/president/news/")

corpus_pre_df <- corpus_df 

corpus_df <- tibble::tibble(section_order = section_order_v) |> 
  dplyr::left_join(corpus_df |> 
  dplyr::mutate(section_order = stringr::str_remove(url, "[[:digit:]]+")),
  by = "section_order") 
 
check <- assertthat::assert_that(nrow(corpus_pre_df)==nrow(corpus_df),
                        msg = "rows dropped when reordering by section")


corpus_df <- corpus_df |> 
  dplyr::distinct(internal_id, .keep_all = TRUE) |> 
  dplyr::arrange(date, internal_id) |> 
  dplyr::select(-section_order)


check <- assertthat::assert_that(0 == corpus_df |> 
                          dplyr::group_by(internal_id) |> 
                          dplyr::count() |> 
                          dplyr::ungroup() |> 
                          dplyr::filter(n>1) |> 
                          nrow(),
                        msg = "duplicated items still present")



corpus_df <- corpus_df |> 
  dplyr::select(-id) |> 
  dplyr::rowwise() |> 
  dplyr::mutate(doc_id = stringr::str_c(website_name, "_", internal_id)) |> 
  dplyr::ungroup() |> 
  dplyr::relocate(doc_id, text, internal_id)

check <- assertthat::assert_that(nrow(corpus_df) == nrow(corpus_df |> dplyr::distinct(doc_id)),
                                 msg = "duplicated doc_id")
```


## Summary statistics

```{r results='asis'}
body_text <- stringr::str_c(
  stringr::str_c("**Dataset name**: ", website_name),
  "**Dataset description**: all items published on the Russian language version of the Kremlin's website",
  paste("**Start date**:", min(corpus_df$date)),
  paste("**End date**:", max(corpus_df$date)),
  paste("**Total items**:", scales::number(nrow(corpus_df))),
  paste("**Available columns**:", colnames(corpus_df) %>% 
          stringr::str_c(collapse = "; ")),
  paste("**License**:", "[Creative Commons Attribution 4.0 International](http://creativecommons.org/licenses/by/4.0/deed.ru)"),
  stringr::str_c("**Link for download**: [", corpus_name, "](https://github.com/giocomai/tadadit/releases/tag/", corpus_name, ")"),
  sep = "\n\n")

cat(body_text)
```

```{r}
#| column: body
#| fig-width: 8
#| fig-height: 4.5
corpus_df |>
  mutate(year = lubridate::year(date)) |> 
  count(year) |> 
  ggplot(mapping = aes(x = year, y = n)) +
  geom_col() +
  scale_y_continuous(name = "", labels = scales::number) +
  scale_x_continuous(name = "", breaks = scales::pretty_breaks(n = 10)) +
  labs(
    title = "Number of items per year published on the Russian-language version of Kremlin.ru",
    subtitle = stringr::str_c(
      "Based on ",
      scales::number(nrow(corpus_df)),
      " items published between ",
      format.Date(x = min(corpus_df$date), "%d %B %Y"), 
      " and ",
      format.Date(x = max(corpus_df$date), "%d %B %Y")),
    caption = "Source: Giorgio Comai / tadadit.xyz"
  )
```

```{r}
words_per_day_df <- corpus_df |> 
  cas_count_total_words() |> 
  mutate(date = lubridate::as_date(date),
         pattern = "total words")

words_per_day_df |> 
  cas_summarise(period = "year", auto_convert = TRUE) |>
  rename(year = date) |> 
  ggplot(mapping = aes(x = year, y = n)) +
  geom_col() +
  scale_y_continuous(name = "", labels = scales::number) +
  scale_x_continuous(name = "", breaks = scales::pretty_breaks(n = 10)) +
  labs(title = "Number of words per year published on the Russian-language version of Kremlin.ru",
       subtitle = stringr::str_c("Based on ",
                                 scales::number(nrow(corpus_df)),
                                 " items published between ",
                                 format.Date(x = min(corpus_df$date), "%d %B %Y"), 
                                 " and ",
                                 format.Date(x = max(corpus_df$date), "%d %B %Y")),
       caption = "Source: Giorgio Comai / tadadit.xyz")
```



```{r piggyback, eval = FALSE}

corpus_path <- fs::path(fs::path_home_r(), 
                      "R",
                      "castarter_2024",
                      "corpora")

fs::dir_create(corpus_path)

release_file <- fs::path(corpus_path, 
                         stringr::str_c(corpus_name, ".csv.gz"))

corpus_df |> 
  readr::write_csv(file = release_file)


piggyback::pb_new_release(repo = "giocomai/tadadit",
               tag = corpus_name,
               body = body_text)

piggyback::pb_upload(file = release_file,
          repo = "giocomai/tadadit",
          tag = corpus_name)

ods_file <- fs::path(corpus_path, 
                         stringr::str_c(path = corpus_name, ".ods"))
                      
readODS::write_ods(x = corpus_df, path = ods_file)

piggyback::pb_upload(file = ods_file,
          repo = "giocomai/tadadit",
          tag = corpus_name)
```


